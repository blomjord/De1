{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f76edd-d541-4bac-a35f-b137fa62827c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018a6d7b-8282-43b7-9642-072789048176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/21 21:40:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import re\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"A3_AN_Spark\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 8)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ee01b6-ed37-4fce-8dc5-ce6180d7bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 21:40:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/02/21 21:40:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/02/21 21:40:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on host-192-168-2-170-de1:10005 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/02/21 21:40:48 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/02/21 21:40:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 221.5 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on host-192-168-2-170-de1:10005 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:48 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/02/21 21:40:48 INFO FileInputFormat: Total input files to process : 1\n",
      "24/02/21 21:40:48 INFO SparkContext: Starting job: count at /tmp/ipykernel_18179/3277269576.py:4\n",
      "24/02/21 21:40:48 INFO DAGScheduler: Got job 0 (count at /tmp/ipykernel_18179/3277269576.py:4) with 3 output partitions\n",
      "24/02/21 21:40:48 INFO DAGScheduler: Final stage: ResultStage 0 (count at /tmp/ipykernel_18179/3277269576.py:4)\n",
      "24/02/21 21:40:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 21:40:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 21:40:48 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[4] at count at /tmp/ipykernel_18179/3277269576.py:4), which has no missing parents\n",
      "24/02/21 21:40:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.1 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on host-192-168-2-170-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:48 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:40:48 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (PythonRDD[4] at count at /tmp/ipykernel_18179/3277269576.py:4) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 21:40:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks resource profile 0\n",
      "24/02/21 21:40:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240221214047-1379/0 on worker-20240221110525-192.168.2.254-35803 (192.168.2.254:35803) with 8 core(s)\n",
      "24/02/21 21:40:49 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/02/21 21:40:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20240221214047-1379/0 on hostPort 192.168.2.254:35803 with 8 core(s), 1024.0 MiB RAM\n",
      "24/02/21 21:40:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240221214047-1379/0 is now RUNNING\n",
      "24/02/21 21:40:51 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.254:54258) with ID 0,  ResourceProfileId 0\n",
      "24/02/21 21:40:51 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "24/02/21 21:40:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.254:10005 with 434.4 MiB RAM, BlockManagerId(0, 192.168.2.254, 10005, None)\n",
      "24/02/21 21:40:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.254, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 21:40:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (192.168.2.254, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/21 21:40:52 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (192.168.2.254, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/02/21 21:40:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.254:10005 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/21 21:40:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.254:10005 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/02/21 21:40:53 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1776 ms on 192.168.2.254 (executor 0) (1/3)\n",
      "24/02/21 21:40:53 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39045\n",
      "24/02/21 21:40:54 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2831 ms on 192.168.2.254 (executor 0) (2/3)\n",
      "24/02/21 21:40:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2890 ms on 192.168.2.254 (executor 0) (3/3)\n",
      "24/02/21 21:40:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:40:54 INFO DAGScheduler: ResultStage 0 (count at /tmp/ipykernel_18179/3277269576.py:4) finished in 6.135 s\n",
      "24/02/21 21:40:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:40:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/02/21 21:40:54 INFO DAGScheduler: Job 0 finished: count at /tmp/ipykernel_18179/3277269576.py:4, took 6.180031 s\n",
      "24/02/21 21:40:54 INFO FileInputFormat: Total input files to process : 1        \n",
      "24/02/21 21:40:54 INFO NetworkTopology: Adding a new node: /default-rack/192.168.2.250:9866\n",
      "24/02/21 21:40:54 INFO SparkContext: Starting job: count at /tmp/ipykernel_18179/3277269576.py:5\n",
      "24/02/21 21:40:54 INFO DAGScheduler: Got job 1 (count at /tmp/ipykernel_18179/3277269576.py:5) with 2 output partitions\n",
      "24/02/21 21:40:54 INFO DAGScheduler: Final stage: ResultStage 1 (count at /tmp/ipykernel_18179/3277269576.py:5)\n",
      "24/02/21 21:40:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 21:40:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 21:40:54 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_18179/3277269576.py:5), which has no missing parents\n",
      "24/02/21 21:40:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.1 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on host-192-168-2-170-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:54 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:40:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[5] at count at /tmp/ipykernel_18179/3277269576.py:5) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 21:40:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "24/02/21 21:40:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (192.168.2.254, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 21:40:54 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4) (192.168.2.254, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/21 21:40:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.254:10005 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "24/02/21 21:40:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.254:10005 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:55 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 808 ms on 192.168.2.254 (executor 0) (1/2)\n",
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines SV: 1862234, Lines EN: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 21:40:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 830 ms on 192.168.2.254 (executor 0) (2/2)\n",
      "24/02/21 21:40:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:40:55 INFO DAGScheduler: ResultStage 1 (count at /tmp/ipykernel_18179/3277269576.py:5) finished in 0.841 s\n",
      "24/02/21 21:40:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:40:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/02/21 21:40:55 INFO DAGScheduler: Job 1 finished: count at /tmp/ipykernel_18179/3277269576.py:5, took 0.849882 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read a file from local filesystem oif your driver\n",
    "fileSV = spark_context.textFile('hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv')\n",
    "fileEN = spark_context.textFile('hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en')\n",
    "linesSV = fileSV.count()\n",
    "linesEN = fileEN.count()\n",
    "print(f\"Lines SV: {linesSV}, Lines EN: {linesEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc4a35d-0ceb-4d92-ace5-b569007f1ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "numPartitions = fileSV.getNumPartitions()\n",
    "print(numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf8c37c-02e9-44f4-9d21-b18d9c7f1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercaseSV = fileSV.map(lambda x: x.lower())\n",
    "lowercaseEN = fileEN.map(lambda x: x.lower())\n",
    "tokenSV = lowercaseSV.map(lambda x: x.split())\n",
    "tokenEN = lowercaseEN.map(lambda x: x.split())\n",
    "#regex = re.compile(r\"\\b\\w+(?:'\\w+)?\\b\")\n",
    "#tokenSV = lowercaseSV.flatMap(lambda x: regex.findall(x))\n",
    "#tokenEN = lowercaseEN.flatMap(lambda x: regex.findall(x))\n",
    "#tokenSV = lowercaseSV.flatMap(lambda x: re.sub(r'[^\\w\\s]', '', x).split())\n",
    "#tokenEN = lowercaseEN.flatMap(lambda x: re.sub(r'[^\\w\\s]', '', x).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c0533d-e727-4432-b372-676312ec0b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 21:40:55 INFO SparkContext: Starting job: count at /tmp/ipykernel_18179/1381452307.py:1\n",
      "24/02/21 21:40:55 INFO DAGScheduler: Got job 2 (count at /tmp/ipykernel_18179/1381452307.py:1) with 3 output partitions\n",
      "24/02/21 21:40:55 INFO DAGScheduler: Final stage: ResultStage 2 (count at /tmp/ipykernel_18179/1381452307.py:1)\n",
      "24/02/21 21:40:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 21:40:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 21:40:55 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[6] at count at /tmp/ipykernel_18179/1381452307.py:1), which has no missing parents\n",
      "24/02/21 21:40:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.9 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on host-192-168-2-170-de1:10005 (size: 5.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:40:55 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (PythonRDD[6] at count at /tmp/ipykernel_18179/1381452307.py:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 21:40:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks resource profile 0\n",
      "24/02/21 21:40:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (192.168.2.254, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 21:40:55 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (192.168.2.254, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/21 21:40:55 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (192.168.2.254, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/02/21 21:40:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.254:10005 (size: 5.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:56 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 486 ms on 192.168.2.254 (executor 0) (1/3)\n",
      "24/02/21 21:40:59 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 3079 ms on 192.168.2.254 (executor 0) (2/3)\n",
      "24/02/21 21:40:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 3317 ms on 192.168.2.254 (executor 0) (3/3)\n",
      "24/02/21 21:40:59 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:40:59 INFO DAGScheduler: ResultStage 2 (count at /tmp/ipykernel_18179/1381452307.py:1) finished in 3.326 s\n",
      "24/02/21 21:40:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:40:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/02/21 21:40:59 INFO DAGScheduler: Job 2 finished: count at /tmp/ipykernel_18179/1381452307.py:1, took 3.329699 s\n",
      "24/02/21 21:40:59 INFO SparkContext: Starting job: count at /tmp/ipykernel_18179/1381452307.py:2\n",
      "24/02/21 21:40:59 INFO DAGScheduler: Got job 3 (count at /tmp/ipykernel_18179/1381452307.py:2) with 2 output partitions\n",
      "24/02/21 21:40:59 INFO DAGScheduler: Final stage: ResultStage 3 (count at /tmp/ipykernel_18179/1381452307.py:2)\n",
      "24/02/21 21:40:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/21 21:40:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 21:40:59 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at count at /tmp/ipykernel_18179/1381452307.py:2), which has no missing parents\n",
      "24/02/21 21:40:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.9 KiB, free 433.9 MiB)\n",
      "24/02/21 21:40:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 433.8 MiB)\n",
      "24/02/21 21:40:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on host-192-168-2-170-de1:10005 (size: 5.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:40:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at count at /tmp/ipykernel_18179/1381452307.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 21:40:59 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
      "24/02/21 21:40:59 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8) (192.168.2.254, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/02/21 21:40:59 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 9) (192.168.2.254, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/02/21 21:40:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.254:10005 (size: 5.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on host-192-168-2-170-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.2.254:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:59 INFO BlockManagerInfo: Removed broadcast_4_piece0 on host-192-168-2-170-de1:10005 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:59 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.254:10005 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:59 INFO BlockManagerInfo: Removed broadcast_3_piece0 on host-192-168-2-170-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:40:59 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.254:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1862234\n",
      "1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 21:41:01 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 2334 ms on 192.168.2.254 (executor 0) (1/2)\n",
      "24/02/21 21:41:01 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 9) in 2340 ms on 192.168.2.254 (executor 0) (2/2)\n",
      "24/02/21 21:41:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:01 INFO DAGScheduler: ResultStage 3 (count at /tmp/ipykernel_18179/1381452307.py:2) finished in 2.358 s\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:41:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Job 3 finished: count at /tmp/ipykernel_18179/1381452307.py:2, took 2.363198 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "verifyLinesSV = tokenSV.count()\n",
    "verifyLinesEN = tokenEN.count()\n",
    "print(verifyLinesSV)\n",
    "print(verifyLinesEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e5b2ea-42be-4081-aaa0-641ecdb9893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 21:41:01 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_18179/291135609.py:5\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Registering RDD 9 (reduceByKey at /tmp/ipykernel_18179/291135609.py:3) as input to shuffle 0\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Got job 4 (sortBy at /tmp/ipykernel_18179/291135609.py:5) with 3 output partitions\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Final stage: ResultStage 5 (sortBy at /tmp/ipykernel_18179/291135609.py:5)\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Submitting ShuffleMapStage 4 (PairwiseRDD[9] at reduceByKey at /tmp/ipykernel_18179/291135609.py:3), which has no missing parents\n",
      "24/02/21 21:41:01 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 13.1 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on host-192-168-2-170-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:01 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:01 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 4 (PairwiseRDD[9] at reduceByKey at /tmp/ipykernel_18179/291135609.py:3) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 21:41:01 INFO TaskSchedulerImpl: Adding task set 4.0 with 3 tasks resource profile 0\n",
      "24/02/21 21:41:01 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 10) (192.168.2.254, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/02/21 21:41:01 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 11) (192.168.2.254, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/02/21 21:41:01 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 12) (192.168.2.254, executor 0, partition 2, ANY, 7679 bytes) \n",
      "24/02/21 21:41:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.254:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:04 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 12) in 2755 ms on 192.168.2.254 (executor 0) (1/3)\n",
      "24/02/21 21:41:20 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 11) in 18270 ms on 192.168.2.254 (executor 0) (2/3)\n",
      "24/02/21 21:41:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 10) in 18921 ms on 192.168.2.254 (executor 0) (3/3)\n",
      "24/02/21 21:41:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:20 INFO DAGScheduler: ShuffleMapStage 4 (reduceByKey at /tmp/ipykernel_18179/291135609.py:3) finished in 18.942 s\n",
      "24/02/21 21:41:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/21 21:41:20 INFO DAGScheduler: running: Set()\n",
      "24/02/21 21:41:20 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
      "24/02/21 21:41:20 INFO DAGScheduler: failed: Set()\n",
      "24/02/21 21:41:20 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[16] at sortBy at /tmp/ipykernel_18179/291135609.py:5), which has no missing parents\n",
      "24/02/21 21:41:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.8 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on host-192-168-2-170-de1:10005 (size: 6.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:20 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (PythonRDD[16] at sortBy at /tmp/ipykernel_18179/291135609.py:5) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 21:41:20 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
      "24/02/21 21:41:20 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 13) (192.168.2.254, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:20 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 14) (192.168.2.254, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:20 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 15) (192.168.2.254, executor 0, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.254:10005 (size: 6.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.254:54258\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 13) in 314 ms on 192.168.2.254 (executor 0) (1/3)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 14) in 311 ms on 192.168.2.254 (executor 0) (2/3)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 15) in 343 ms on 192.168.2.254 (executor 0) (3/3)\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:21 INFO DAGScheduler: ResultStage 5 (sortBy at /tmp/ipykernel_18179/291135609.py:5) finished in 0.361 s\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Job 4 finished: sortBy at /tmp/ipykernel_18179/291135609.py:5, took 19.356522 s\n",
      "24/02/21 21:41:21 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_18179/291135609.py:5\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Got job 5 (sortBy at /tmp/ipykernel_18179/291135609.py:5) with 3 output partitions\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Final stage: ResultStage 7 (sortBy at /tmp/ipykernel_18179/291135609.py:5)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[17] at sortBy at /tmp/ipykernel_18179/291135609.py:5), which has no missing parents\n",
      "24/02/21 21:41:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 11.1 KiB, free 433.8 MiB)\n",
      "24/02/21 21:41:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.8 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on host-192-168-2-170-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_7_piece0 on host-192-168-2-170-de1:10005 in memory (size: 6.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.254:10005 in memory (size: 6.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (PythonRDD[17] at sortBy at /tmp/ipykernel_18179/291135609.py:5) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks resource profile 0\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 16) (192.168.2.254, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 17) (192.168.2.254, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 18) (192.168.2.254, executor 0, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on host-192-168-2-170-de1:10005 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.254:10005 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_6_piece0 on host-192-168-2-170-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.254:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.254:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 18) in 198 ms on 192.168.2.254 (executor 0) (1/3)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 17) in 218 ms on 192.168.2.254 (executor 0) (2/3)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 16) in 226 ms on 192.168.2.254 (executor 0) (3/3)\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:21 INFO DAGScheduler: ResultStage 7 (sortBy at /tmp/ipykernel_18179/291135609.py:5) finished in 0.248 s\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Job 5 finished: sortBy at /tmp/ipykernel_18179/291135609.py:5, took 0.254248 s\n",
      "24/02/21 21:41:21 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Registering RDD 19 (sortBy at /tmp/ipykernel_18179/291135609.py:5) as input to shuffle 1\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Got job 6 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Final stage: ResultStage 10 (runJob at PythonRDD.scala:181)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Submitting ShuffleMapStage 9 (PairwiseRDD[19] at sortBy at /tmp/ipykernel_18179/291135609.py:5), which has no missing parents\n",
      "24/02/21 21:41:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.9 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on host-192-168-2-170-de1:10005 (size: 7.3 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 9 (PairwiseRDD[19] at sortBy at /tmp/ipykernel_18179/291135609.py:5) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Adding task set 9.0 with 3 tasks resource profile 0\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 19) (192.168.2.254, executor 0, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 20) (192.168.2.254, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 21) (192.168.2.254, executor 0, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.254:10005 (size: 7.3 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 19) in 283 ms on 192.168.2.254 (executor 0) (1/3)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 21) in 309 ms on 192.168.2.254 (executor 0) (2/3)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 20) in 310 ms on 192.168.2.254 (executor 0) (3/3)\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:21 INFO DAGScheduler: ShuffleMapStage 9 (sortBy at /tmp/ipykernel_18179/291135609.py:5) finished in 0.319 s\n",
      "24/02/21 21:41:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/21 21:41:21 INFO DAGScheduler: running: Set()\n",
      "24/02/21 21:41:21 INFO DAGScheduler: waiting: Set(ResultStage 10)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: failed: Set()\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/21 21:41:21 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.9 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:21 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on host-192-168-2-170-de1:10005 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (PythonRDD[22] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22) (192.168.2.254, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.2.254:10005 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.254:54258\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_8_piece0 on host-192-168-2-170-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.254:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_9_piece0 on host-192-168-2-170-de1:10005 in memory (size: 7.3 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.2.254:10005 in memory (size: 7.3 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 136 ms on 192.168.2.254 (executor 0) (1/1)\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:21 INFO DAGScheduler: ResultStage 10 (runJob at PythonRDD.scala:181) finished in 0.145 s\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Job 6 finished: runJob at PythonRDD.scala:181, took 0.475100 s\n",
      "24/02/21 21:41:21 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_18179/291135609.py:6\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Registering RDD 13 (reduceByKey at /tmp/ipykernel_18179/291135609.py:4) as input to shuffle 2\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Got job 7 (sortBy at /tmp/ipykernel_18179/291135609.py:6) with 2 output partitions\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Final stage: ResultStage 12 (sortBy at /tmp/ipykernel_18179/291135609.py:6)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 11)\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Submitting ShuffleMapStage 11 (PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_18179/291135609.py:4), which has no missing parents\n",
      "24/02/21 21:41:21 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.1 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:21 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on host-192-168-2-170-de1:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:21 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:21 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 11 (PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_18179/291135609.py:4) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 21:41:21 INFO TaskSchedulerImpl: Adding task set 11.0 with 2 tasks resource profile 0\n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 23) (192.168.2.254, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/02/21 21:41:21 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 24) (192.168.2.254, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/02/21 21:41:21 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.254:10005 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:40 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 23) in 19075 ms on 192.168.2.254 (executor 0) (1/2)\n",
      "24/02/21 21:41:41 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 24) in 19800 ms on 192.168.2.254 (executor 0) (2/2)\n",
      "24/02/21 21:41:41 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:41 INFO DAGScheduler: ShuffleMapStage 11 (reduceByKey at /tmp/ipykernel_18179/291135609.py:4) finished in 19.809 s\n",
      "24/02/21 21:41:41 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/21 21:41:41 INFO DAGScheduler: running: Set()\n",
      "24/02/21 21:41:41 INFO DAGScheduler: waiting: Set(ResultStage 12)\n",
      "24/02/21 21:41:41 INFO DAGScheduler: failed: Set()\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Submitting ResultStage 12 (PythonRDD[23] at sortBy at /tmp/ipykernel_18179/291135609.py:6), which has no missing parents\n",
      "24/02/21 21:41:41 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 11.8 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:41 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 433.8 MiB)\n",
      "24/02/21 21:41:41 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on host-192-168-2-170-de1:10005 (size: 6.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:41 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 12 (PythonRDD[23] at sortBy at /tmp/ipykernel_18179/291135609.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 21:41:41 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks resource profile 0\n",
      "24/02/21 21:41:41 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 25) (192.168.2.254, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:41 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 26) (192.168.2.254, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:41 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.2.254:10005 (size: 6.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.254:54258\n",
      "24/02/21 21:41:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on host-192-168-2-170-de1:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.2.254:10005 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on host-192-168-2-170-de1:10005 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.2.254:10005 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:41 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 25) in 97 ms on 192.168.2.254 (executor 0) (1/2)\n",
      "24/02/21 21:41:41 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 26) in 101 ms on 192.168.2.254 (executor 0) (2/2)\n",
      "24/02/21 21:41:41 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:41 INFO DAGScheduler: ResultStage 12 (sortBy at /tmp/ipykernel_18179/291135609.py:6) finished in 0.110 s\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:41:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Job 7 finished: sortBy at /tmp/ipykernel_18179/291135609.py:6, took 19.928803 s\n",
      "24/02/21 21:41:41 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_18179/291135609.py:6\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Got job 8 (sortBy at /tmp/ipykernel_18179/291135609.py:6) with 2 output partitions\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Final stage: ResultStage 14 (sortBy at /tmp/ipykernel_18179/291135609.py:6)\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Submitting ResultStage 14 (PythonRDD[24] at sortBy at /tmp/ipykernel_18179/291135609.py:6), which has no missing parents\n",
      "24/02/21 21:41:41 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 11.1 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:41 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:41 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on host-192-168-2-170-de1:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:41 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 14 (PythonRDD[24] at sortBy at /tmp/ipykernel_18179/291135609.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 21:41:41 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0\n",
      "24/02/21 21:41:41 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 27) (192.168.2.254, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:41 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 28) (192.168.2.254, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/02/21 21:41:41 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.2.254:10005 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:41 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 28) in 93 ms on 192.168.2.254 (executor 0) (1/2)\n",
      "24/02/21 21:41:41 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 27) in 95 ms on 192.168.2.254 (executor 0) (2/2)\n",
      "24/02/21 21:41:41 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:41 INFO DAGScheduler: ResultStage 14 (sortBy at /tmp/ipykernel_18179/291135609.py:6) finished in 0.105 s\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:41:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Job 8 finished: sortBy at /tmp/ipykernel_18179/291135609.py:6, took 0.109250 s\n",
      "24/02/21 21:41:41 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Registering RDD 26 (sortBy at /tmp/ipykernel_18179/291135609.py:6) as input to shuffle 3\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Got job 9 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Final stage: ResultStage 17 (runJob at PythonRDD.scala:181)\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)\n",
      "24/02/21 21:41:41 INFO DAGScheduler: Submitting ShuffleMapStage 16 (PairwiseRDD[26] at sortBy at /tmp/ipykernel_18179/291135609.py:6), which has no missing parents\n",
      "24/02/21 21:41:41 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 11.9 KiB, free 433.9 MiB)\n",
      "24/02/21 21:41:42 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 433.8 MiB)\n",
      "24/02/21 21:41:42 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on host-192-168-2-170-de1:10005 (size: 7.3 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:42 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:42 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (PairwiseRDD[26] at sortBy at /tmp/ipykernel_18179/291135609.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/02/21 21:41:42 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks resource profile 0\n",
      "24/02/21 21:41:42 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 29) (192.168.2.254, executor 0, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/02/21 21:41:42 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 30) (192.168.2.254, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/02/21 21:41:42 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.2.254:10005 (size: 7.3 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:42 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 30) in 139 ms on 192.168.2.254 (executor 0) (1/2)\n",
      "24/02/21 21:41:42 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 29) in 140 ms on 192.168.2.254 (executor 0) (2/2)\n",
      "24/02/21 21:41:42 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:42 INFO DAGScheduler: ShuffleMapStage 16 (sortBy at /tmp/ipykernel_18179/291135609.py:6) finished in 0.149 s\n",
      "24/02/21 21:41:42 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/02/21 21:41:42 INFO DAGScheduler: running: Set()\n",
      "24/02/21 21:41:42 INFO DAGScheduler: waiting: Set(ResultStage 17)\n",
      "24/02/21 21:41:42 INFO DAGScheduler: failed: Set()\n",
      "24/02/21 21:41:42 INFO DAGScheduler: Submitting ResultStage 17 (PythonRDD[29] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/02/21 21:41:42 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 9.9 KiB, free 433.8 MiB)\n",
      "24/02/21 21:41:42 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.8 MiB)\n",
      "24/02/21 21:41:42 INFO BlockManagerInfo: Removed broadcast_12_piece0 on host-192-168-2-170-de1:10005 in memory (size: 6.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:42 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on host-192-168-2-170-de1:10005 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:42 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/02/21 21:41:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (PythonRDD[29] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/21 21:41:42 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "24/02/21 21:41:42 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.2.254:10005 in memory (size: 6.7 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:42 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 31) (192.168.2.254, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('att', 1709969), ('och', 1351079), ('i', 1054764), ('det', 953045), ('som', 917621), ('för', 915166), ('av', 740770), ('är', 701851), ('en', 636939), ('vi', 546126)]\n",
      "[('the', 3506102), ('of', 1662891), ('to', 1545280), ('and', 1320053), ('in', 1099408), ('that', 839196), ('a', 776918), ('is', 774969), ('for', 538487), ('we', 526580)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 21:41:42 INFO BlockManagerInfo: Removed broadcast_13_piece0 on host-192-168-2-170-de1:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:42 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.254:10005 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:42 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.2.254:10005 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "24/02/21 21:41:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.2.254:54258\n",
      "24/02/21 21:41:42 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 31) in 77 ms on 192.168.2.254 (executor 0) (1/1)\n",
      "24/02/21 21:41:42 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/02/21 21:41:42 INFO DAGScheduler: ResultStage 17 (runJob at PythonRDD.scala:181) finished in 0.088 s\n",
      "24/02/21 21:41:42 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/21 21:41:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/02/21 21:41:42 INFO DAGScheduler: Job 9 finished: runJob at PythonRDD.scala:181, took 0.245912 s\n"
     ]
    }
   ],
   "source": [
    "reTokenSV = lowercaseSV.flatMap(lambda x: re.findall(r\"\\b\\w+\\b\", x))\n",
    "reTokenEN = lowercaseEN.flatMap(lambda x: re.findall(r\"\\b\\w+\\b\", x))\n",
    "nWordsSV = reTokenSV.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "nWordsEN = reTokenEN.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "tenMostCommonSV = nWordsSV.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "tenMostCommonEN = nWordsEN.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "print(tenMostCommonSV)\n",
    "print(tenMostCommonEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb628b37-5bab-4d22-8990-b2d079390330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The top ten most common words generated reflect what is the most commond words in reality as well\n"
     ]
    }
   ],
   "source": [
    "# The top ten most common words generated reflect what is the most commond words in reality as well\n",
    "print(\"# The top ten most common words generated reflect what is the most commond words in reality as well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e6f01ee-83e8-46c1-851e-bcc826b05fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3650069181.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    print_9 = wordPairs.sortByKey(ascending=False)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/21 22:10:47 INFO BlockManagerInfo: Removed broadcast_17_piece0 on host-192-168-2-170-de1:10005 in memory (size: 5.4 KiB, free: 434.3 MiB)\n",
      "24/02/21 22:10:47 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.2.254:10005 in memory (size: 5.4 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "#print(tokenSV.take(5))\n",
    "sv_1 = tokenSV.flatMap(lambda x: x)\n",
    "en_1 = tokenEN.flatMap(lambda x: x)\n",
    "sv_1 = sv_1.map(lambda x: ' '.join(x))\n",
    "en_1 = en_1.map(lambda x: ' '.join(x))\n",
    "sv_1 = sv_1.zipWithIndex()\n",
    "en_1 = en_1.zipWithIndex()\n",
    "sv_2 = sv_1.map(lambda x: (x[1], [0]))\n",
    "en_2 = en_1.map(lambda x: (x[1], [0]))\n",
    "joined_3 = sv_2.join(en_2)\n",
    "filter_4 = joined_3.filter(lambda x: x[1][0] and x[1][1])\n",
    "filter_5 = filter_4.filter(lambda x: len(x[1][0].split()) <= 8 and len(x[1][1].split()) <= 8)\n",
    "filter_6 = filter_5.filter(lambda x: len(x[1][0].split()) == len(x[1][1].split()))\n",
    "map_7 = filter_6.map(lambda x: (zip(x[1][0].split(), x[1][1].split())))\n",
    "reduce_8 = map_7.flatMap(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5dd99a-2fab-43aa-90a2-b9823129e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
